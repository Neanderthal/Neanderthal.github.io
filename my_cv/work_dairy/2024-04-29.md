How to run model on server :
```bash
docker run --rm --gpus all --shm-size 1g -p 8090:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:2.0  --model-id $model
```

How send request to that model:
```bash
curl localhost:8090/generate_stream     -X POST     -d '{"inputs":"What is Deep Learning?","parameters":{"max_new_tokens":20}}'     -H 'Content-Type: application/json'
```

From python:
```python
import requests

headers = {
    "Content-Type": "application/json",
}

data = {
    'inputs': 'What is Deep Learning?',
    'parameters': {
        'max_new_tokens': 20,
    },
}

response = requests.post('http://127.0.0.1:8090/generate', headers=headers, json=data)
print(response.json())
```

Now I have to check parameters on container 
```bash
docker run ghcr.io/huggingface/text-generation-inference:2.0 --help
```

And  parameters  you can push with data:
["https://huggingface.co/docs/text-generation-inference/messages_api"](api description)

After that I have to compare parameters of all three or for llms with the help of chatgpt
